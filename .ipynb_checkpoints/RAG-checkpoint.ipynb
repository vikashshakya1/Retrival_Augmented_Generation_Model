{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbc8e26-27ec-4d8f-90f2-86341617ffab",
   "metadata": {},
   "source": [
    "# Import the libraries, which we are going to use in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c430eb-9085-44ab-9875-89b44a4d5c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a018638-3c43-4bae-84a1-4e04abdf51f3",
   "metadata": {},
   "source": [
    "# Document Loading\r",
    "*  Using Hugging Face, load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfec5e5e-a695-4d12-8163-007013fc7abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"'),\n",
       " Document(metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'}, page_content='\"\"')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the dataset name and the column containing the content\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"  # or any other column you're interested in\n",
    "\n",
    "# Create a loader instance\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "\n",
    "# Load the data\n",
    "data = loader.load()\n",
    "\n",
    "# Display the first 15 entries\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedc8e3-c8d5-47bf-afb6-cca5eaf67d3d",
   "metadata": {},
   "source": [
    "# Document Transformers\r",
    "* There are several “Text Splitters” in LangChain, you have to choose according to your choice.\n",
    "* I chose “RecursiveCharacterTextSplitter”. This text splitter is recommended for generic text.\n",
    "* It is parametrized by a list of characters.\n",
    "* It tries to split the long texts recursively until the chunks are smaller enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af374c5-53ef-4c2b-bc49-a37d6192e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
    "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384cd2e7-35ba-4e05-8372-7064ab7425c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c72edd-f44c-4dea-a0c8-784bb04bb74d",
   "metadata": {},
   "source": [
    "# Text Embedding\r",
    "* Embeddings capture the semantic meaning of the text which allows you to quickly and efficiently find other pieces of text which are similar.4\n",
    "* The Embeddings class of LangChain is designed for interfacing with text embedding models.\n",
    "* You can use any of them, but I have used here “HuggingFaceEmbeddings”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690bbf56-8dea-4de2-b0c2-148dd5cb3848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2448\\3072746259.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478d6191-2d53-4bda-80fc-49d860ac98a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03833850845694542, 0.1234646737575531, -0.028642946854233742]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfe38f-5949-49b3-96d0-d5b44866ee60",
   "metadata": {},
   "source": [
    "# Vector Stores\n",
    "* There is a need of databases so that we can store those embeddings and efficiently search them.\n",
    "* Therefore, for storage and searching purpose, we need vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba2466-f9b3-4c22-a581-ba025a0ac28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555032b-425d-4ad9-a6f5-4765013c4489",
   "metadata": {},
   "source": [
    "Now, search the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac72f5-006d-40b5-b8d6-011ba5c56728",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is cheesemaking?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03c00e-c085-4055-b4ad-5f40a276a4d3",
   "metadata": {},
   "source": [
    "# Preparing the LLM Model\r",
    "* We can choose any model from hugging face, and start with a tokenizer to preprocess text and a question-answering model to provide answers based on input text and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf71db9-6876-4b66-ba3e-70f878cb379e",
   "metadata": {},
   "source": [
    "* I used Intel/dynamic_tinybert which is a fine-tuned model for the purpose of question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b63e0-bcf7-43d5-89d2-0864846bf391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcf328-2a07-4f60-81b9-e5cb459792f3",
   "metadata": {},
   "source": [
    "* Create a question-answering pipeline using your pre-trained model and tokenizer and then extend its functionality by creating a LangChain pipeline with additional model-specific arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7e69b-7a20-4b47-85a1-cb92fb2406fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model name you want to use\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e797246-e4ea-4896-a3d1-400f95ca1287",
   "metadata": {},
   "source": [
    "# Retrievers\n",
    "Once the data is in database, the LLM model is prepared, and the pipeline is created, we need to retrieve the data. A retriever is an interface that returns documents from the query.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd3775-86f5-48de-a547-9f3cc2e44561",
   "metadata": {},
   "source": [
    "It is not able to store the documents, only return or retrieves them. Basically, vector stores are the backbone of the retrievers. There are many retriever algorithms in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d452fb7-72d8-4753-b8c0-bf5005ff0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
    "# This retriever is likely used for retrieving data or documents from the database.\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e085fa-4202-47d3-a915-e06f2c9a6314",
   "metadata": {},
   "source": [
    "Searching relevant documents for the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8ef6a-c604-4684-b652-d6a2a00377e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Cheesemaking?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115af61-73ce-4c41-82ae-252d9a98c984",
   "metadata": {},
   "source": [
    "# Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3b7fb-94bd-4337-9b86-1f515758a335",
   "metadata": {},
   "source": [
    "* Now, we’re going to use a RetrievalQA chain to find the answer to a question.\n",
    "* To do this, we prepared our LLM model with “temperature = 0.7\" and “max_length = 512”.\n",
    "* You can set your temperature whatever you desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900ee02-5909-41bd-812f-a31232c49385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
    "# It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee9894-d590-4cbd-b8ce-3df6184ea295",
   "metadata": {},
   "source": [
    "Finally, we call this QA chain with the question we want to ask.\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1a456-747f-47e2-a043-3e7bf7a396d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Provide the context and question\n",
    "context = (\n",
    "    \"Thomas Jefferson (April 13, 1743 – July 4, 1826) was an American statesman, diplomat, lawyer, \"\n",
    "    \"architect, philosopher, and Founding Father who served as the third president of the United States \"\n",
    "    \"from 1801 to 1809. Among the Committee of Five charged by the Second Continental Congress with authoring \"\n",
    "    \"the Declaration of Independence, Jefferson was the Declaration's primary author. Following the American \"\n",
    "    \"Revolutionary War and prior to becoming the nation's third president in 1801, Jefferson was the first \"\n",
    "    \"United States secretary of state under George Washington and then the nation's second vice president under John Adams.\"\n",
    ")\n",
    "\n",
    "question = \"Who is Thomas Jefferson?\"\n",
    "\n",
    "# Run the QA pipeline\n",
    "result = qa_pipeline({\"context\": context, \"question\": question})\n",
    "\n",
    "# Print the answer\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ec98e-f8a3-4666-a2a1-1ee27cd13257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
